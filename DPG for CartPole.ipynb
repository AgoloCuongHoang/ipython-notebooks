{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict, namedtuple\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import theano as th\n",
    "import theano.tensor as T\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.cartpole import CartPole\n",
    "from utils import VariableStore, Linear, SGD, momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = int(time.time())\n",
    "print seed\n",
    "rng = T.shared_randomstreams.RandomStreams(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STATE_DIM = 4\n",
    "ACTION_DIM = 1\n",
    "EXPLORE_RANGE = 0.5\n",
    "\n",
    "Critic = namedtuple(\"Critic\", [\"pred\", \"targets\", \"cost\", \"updates\"])\n",
    "\n",
    "\n",
    "class DPGModel(object):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, explore_range=0.5, name=\"dpg\"):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.explore_range = explore_range\n",
    "        self.name = name\n",
    "        \n",
    "        self._vs_actor = VariableStore(\"%s/vs_a\" % name)\n",
    "        self._vs_critic = VariableStore(\"%s/vs_c\" % name)\n",
    "        \n",
    "        self._make_vars()\n",
    "        self._make_graph()\n",
    "        self._make_updates()\n",
    "        self._make_functions()\n",
    "        \n",
    "    def _make_vars(self):\n",
    "        self.X = T.matrix(\"X\")\n",
    "\n",
    "        # Optionally directly provide actions predicted\n",
    "        self.actions = T.matrix(\"actions\")\n",
    "        # Q target values\n",
    "        self.q_targets = T.vector(\"q_targets\")\n",
    "        # Learning rate\n",
    "        self.lr = T.scalar(\"lr\")\n",
    "        \n",
    "    def _make_graph(self):\n",
    "        # Deterministic policy: linear map\n",
    "        self.a_pred = Linear(self.X, self.state_dim, self.action_dim,\n",
    "                             self._vs_actor)\n",
    "\n",
    "        # Exploration policy: add noise\n",
    "        self.a_explore = self.a_pred + rng.uniform(self.a_pred.shape,\n",
    "                                                   -self.explore_range,\n",
    "                                                   self.explore_range, ndim=2)\n",
    "\n",
    "        # Create a few different Critic instances (Q-functions). These all\n",
    "        # share parameters; they only differ in the sources of their inputs.\n",
    "        #\n",
    "        # Critic 1: actions given\n",
    "        self.critic_given = self._make_critic(self.actions, self.q_targets)\n",
    "        # Critic 2: with deterministic policy\n",
    "        self.critic_det = self._make_critic(self.a_pred, self.q_targets)\n",
    "        # Critic 3: with noised / exploration policy\n",
    "        self.critic_exp = self._make_critic(self.a_explore, self.q_targets)\n",
    "        \n",
    "    def _make_critic(self, actions, targets):\n",
    "        # Q-function is a linear map on state+action pair.\n",
    "        q_pred = Linear(T.concatenate([self.X, actions], axis=1),\n",
    "                        self.state_dim + self.action_dim, 1, self._vs_critic,\n",
    "                        \"%s/q\" % self.name)\n",
    "        q_pred = q_pred.reshape((-1,))\n",
    "        \n",
    "        # MSE loss on TD backup targets.\n",
    "        q_cost = ((targets - q_pred) ** 2).mean()\n",
    "        q_updates = momentum(q_cost, self._vs_critic.vars.values(), self.lr)\n",
    "        return Critic(q_pred, targets, q_cost, q_updates)\n",
    "    \n",
    "    def _make_updates(self):\n",
    "        # Actor-critic learning w/ critic 3\n",
    "        # NB, need to flatten all timesteps into a single batch\n",
    "        self.updates = OrderedDict(self.critic_exp.updates)\n",
    "        # Add policy gradient updates\n",
    "        self.updates.update(momentum(-self.critic_exp.pred.mean(),\n",
    "                                     self._vs_actor.vars.values(),\n",
    "                                     self.lr))\n",
    "        \n",
    "    def _make_functions(self):\n",
    "        # On-policy action prediction function\n",
    "        self.f_action_on = th.function([self.X], self.a_pred)\n",
    "        # Off-policy action prediction function\n",
    "        self.f_action_off = th.function([self.X], self.a_explore)\n",
    "\n",
    "        # Q-function\n",
    "        self.f_q = th.function([self.X, self.actions], self.critic_given.pred)\n",
    "\n",
    "        # Actor-critic update\n",
    "        self.f_update = th.function([self.X, self.q_targets, self.lr],\n",
    "                                    (self.critic_exp.cost, self.critic_exp.pred),\n",
    "                                    updates=self.updates)\n",
    "        \n",
    "        \n",
    "dpg = DPGModel(STATE_DIM, ACTION_DIM, EXPLORE_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(f_onpolicy, f_offpolicy, f_q):\n",
    "    \"\"\"\n",
    "    Simulate a trajectory and record states and rewards.\n",
    "    Return a batch of (s, a, r).\n",
    "    \"\"\"\n",
    "    cp = CartPole()\n",
    "    trace = cp.single_episode(policy=lambda *args: f_offpolicy(np.array(args).reshape((1, -1))))\n",
    "    \n",
    "    states, actions, rewards = [], [], []\n",
    "    for state_t, action_t, reward_t, _, _ in trace:\n",
    "        states.append(state_t)\n",
    "        actions.append(0 if action_t < 0 else 1)\n",
    "        rewards.append(reward_t)\n",
    "\n",
    "    states, actions, rewards = np.array(states), np.array(actions).astype(np.int32), np.array(rewards)    \n",
    "    return len(trace), states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep a replay buffer of states, actions, rewards, targets\n",
    "R_states = np.empty((0, STATE_DIM), dtype=th.config.floatX)\n",
    "R_actions = np.empty((0,), dtype=np.int32)\n",
    "R_rewards = np.empty((0,), dtype=np.int32)\n",
    "\n",
    "steps = []\n",
    "q_costs = []\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "LR = 0.01\n",
    "GAMMA = 0.99\n",
    "\n",
    "for t in xrange(1000):\n",
    "    steps_t, states_t, actions_t, rewards_t = run_episode(dpg.f_action_on, dpg.f_action_off, dpg.f_q)\n",
    "    \n",
    "    R_states = np.append(R_states, states_t, axis=0)\n",
    "    R_actions = np.append(R_actions, actions_t)\n",
    "    R_rewards = np.append(R_rewards, rewards_t)\n",
    "    \n",
    "    if len(R_states) - 1 < BATCH_SIZE:\n",
    "        # Not enough data. Keep collecting trajectories.\n",
    "        continue\n",
    "\n",
    "    # Sample a training minibatch.\n",
    "    idxs = np.random.choice(len(R_states) - 1, size=BATCH_SIZE, replace=False)\n",
    "    b_states, b_actions, b_rewards = R_states[idxs], R_actions[idxs], R_rewards[idxs]\n",
    "    \n",
    "    # Compute targets (TD error backups) given current Q function.\n",
    "    next_states = R_states[idxs + 1] # may bork at the end of each trajectory, but I don't care\n",
    "    next_actions = dpg.f_action_on(next_states)\n",
    "    b_targets = b_rewards + GAMMA * dpg.f_q(next_states, next_actions).reshape((-1,))\n",
    "    \n",
    "    # SGD update.\n",
    "    cost_t, _ = dpg.f_update(b_states, b_targets, LR)\n",
    "    \n",
    "    steps.append(steps_t)\n",
    "    q_costs.append(cost_t)\n",
    "    print \"%i\\t% 4i\\t%10f\" % (t, steps_t, cost_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(steps)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Steps until failure\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(q_costs, \"r\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Q cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
